<head>
    <title>Kaichun Mo</title>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="resources/main.css">
</head>

<body>

<table>
<tr>
<td><img src="images/face.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Kaichun Mo 莫凯淳</div>
<div>
Research Scientist at NVIDIA<br/>
</div>
<div>
    <b>Email:</b> <tt>kmo [at] nvidia [dot] com</tt><br>
    <b>Office: </b>4545 Roosevelt Way, 6th Floor, Seattle, WA, 98105<br>
</div>
<div>
<a href="resources/cv.pdf" target='_blank'>[CV (Aug, 2022)]</a>&nbsp;
<a href="#dissertation">[Projects]</a>&nbsp;
<a href="#education">[Education]</a>&nbsp;
<a href="#experiences">[Experiences]</a>&nbsp;
<br><br>
<a href="https://scholar.google.com/citations?user=pL7JsOsAAAAJ&hl=en" target='_blank'>[Google Scholar]</a>&nbsp;
<a href="https://github.com/daerduocarey" target='_blank'>[GitHub]</a>
<a href="https://www.linkedin.com/in/kaichun-mo-a681b296/" target="_blank">[LinkedIn]</a>
<a href="https://twitter.com/KaichunMo" target="_blank">[Twitter]</a>
</div>
</td>
</tr>
</table>
<br>

<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>"The biggest obstacle (in ML/AI development at the moment) is the difficulty for everyone to think differently and question conventional wisdom."</i></p>
        <p>--- Yann LeCun</p>
</div>


<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>"The people who are crazy enough to think they can change the world are the ones who do."</i></p>
        <p>--- Steve Jobs</p>
</div>

<div class="w3-panel w3-leftbar w3-light-grey">
      <p class="w3-xlarge w3-serif">
      <i>白日不到处，青春恰自来。苔花如米小，亦学牡丹开。</i></p>
        <p>--- 《苔》【清】袁枚</p>
</div>


<h3>About</h3>
<div class="section">
<ul>
    <p>
    My research interests include 3D Computer Vision, Graphics, Robotics, and 3D Deep Learning, with particular interests in object-centric 3D deep learning, and structured visual representation learning for 3D data.
    </p>

    <p>
    I am currenty a Research Scientist at 
    <a href="http://nvidia_srl.gitlab.io/" target="_blank">Seattle Robotics Lab</a> under 
    <a href="https://research.nvidia.com/person/dieter-fox" target="_blank">Prof. Dieter Fox</a>, 
    <a href="https://www.nvidia.com/en-us/research/" target="_blank">NVIDIA Research</a>.
    I obtained my Ph.D. in Computer Science from 
<a href="http://cs.stanford.edu" target='_blank'>Stanford University</a>
    advised by 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target='_blank'>Prof. Leonidas J. Guibas</a>.
    I was affiliated with
<a href="http://geometry.stanford.edu" target='_blank'>Geometric Computation Group</a> and 
<a href="http://ai.stanford.edu" target='_blank'>Artificial Intelligence Lab</a> at Stanford.
    Prior to joining Stanford in 2016, I got my Bachelor degree from 
    <a href="http://acm.sjtu.edu.cn/" target="_blank">ACM Honored Class</a>, 
    <a href="http://zhiyuan.sjtu.edu.cn/" target="_blank">Zhiyuan College</a>, 
    <a href="http://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University</a>.
    I got a GPA <b>3.96/4.30 (91.87/100)</b>, ranked <b>1/33</b> in the class.
    </p>
</div>
<br>

<h3>News</h3>
<div class="section">
<ul>
<ul>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Aug, 2023] One paper accepted to CoRL 2023.</li>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Jul, 2023] One paper accepted to ICCV 2023.</li>
    <li><b style="color: green; background-color: #ffff42">NEW</b> [Mar, 2023] We will host <a href="https://ai3dcc.github.io/" target="_blank">The First Workshop on AI for 3D Content Creation (AI3DCC)</a> at ICCV 2023.</li>
    <li>[Apr, 2023] One paper accepted to ICML 2023.</li>
    <li>[Feb, 2023] Call-for-Papers for CVPR 2023 workshops: 
            <a href="https://struco3d.github.io/cvpr2023" target="_blank">Structural and Compositional Learning on 3D Data (StruCo3D)</a>,
            <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">3D Vision and Robotics (3DVR)</a>, and 
            <a href="https://rhobin-challenge.github.io/" target="_blank">The Rhobin Challenge -- Reconstruction of human-object interaction</a>.
    </li>
    <li>[Feb, 2023] Papers accepted to CVPR 2023, ICLR 2023, TPAMI, and WACV 2023.</li>
    <li>[Jul, 2022] Two papers get accepted to ECCV 2022.</li>
    <li>[Jun, 2022] Successfully defended <a href="https://purl.stanford.edu/xn613mb1512" target="_blank">my Ph.D. thesis</a> (featured in <a href="https://www.rsipvision.com/ComputerVisionNews-2022June/22/" target="_blank">Computer Vision News magazine</a>).</li>
    <li>Our ECCV22 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/voli/" target="_blank">VOLI: Visual Object-oriented Learning meets Interaction: Discovery, Representations, and Applications</a>. Please consider submitting your works by July 5th, 2022. See you online in Oct, 2022.</li>
    <li>One paper gets accepted to CVPR 2022.</li>
    <li>[Jan, 2022] Three papers get accepted to ICLR 2022.</li>
    <li>[Sept, 2021] Two papers get accepted to CoRL 2021. My first ever papers in any Robotic Conference!</li>
    <li>[July, 2021] One paper gets accepted to ICCV 2021.</li>
    <li>[Apr, 2021] 
        Our ICCV2021 Workshop Call-for-paper: <a href="https://geometry.stanford.edu/struco3d/" target="_blank">StruCo3D2021: Structural and Compositional Learning on 3D Data</a>. Please consider submitting your works by July 26, 2021. See you online on Oct 16th, 2021.</li>
    <li>[Sep, 2020] One paper gets accepted to NeurIPS 2020.</li>
    <li>[July, 2020] Two papers get accepted to ECCV 2020.</li>
    <li>[Feb, 2020] Two papers get accepted to CVPR 2020.</li>
    <li>[Dec, 2019] One paper gets accepted to ICLR 2020.</li>
    <li>[July, 2019] StructureNet gets accepted to Siggraph Asia 2019!</li>
    <li>[Feb, 2019] PartNet gets accepted to CVPR 2019!</li>
    <li>[February 2017] Our <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">PointNet</a> is accepted as an oral presentation in CVPR 2017, Honolulu, USA.</li>
</ul>
</div>
<br>


<a name="dissertation"></a>
<h3>Dissertation</h3>
<div class="mainsection">
<ul>

<table width="100%">

<tr>
<td width="30%" valign="top"><p><img src="logos/stanford.png" width="200" alt="" style="border-style: none; display: block; margin-left: auto; margin-right: auto;" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        Learning Compositional and Actionable Visual Representations for 3D Shapes
    </h3>
    <strong>Kaichun Mo</strong><br>
    Ph.D. Thesis<br><br>
    <i><b style="color: red;">Featured in: </b>
        <a href="https://www.rsipvision.com/ComputerVisionNews-2022June/22/" target="_blank">Computer Vision News magazine (June 2022)</a>
    </i>
    <p>
    The representation of 3D objects, or 3D shapes, is one of the cornerstones of human-level 3D vision and robotics systems.  This is notoriously challenging given that 3D shapes have extraordinarily diverse geometries, rich functional semantics, and complicated part structures.  In the meantime, the complex nature of 3D tasks in vision, graphics and robotics is another challenge we are facing.
    To this end, this thesis explores to tackle the core problems of building compositional and actionable visual representations for diverse 3D shapes and designing scalable learning frameworks for various 3D tasks.
    </p>
    <a href="https://purl.stanford.edu/xn613mb1512" target="_blank">[Paper]</a>
</p></td>
</tr>

</table>
</ul>
</div>
<br>


<a name="manuscripts"></a>
<h3>Manuscripts</h3>
<div class="mainsection">
<ul>

(<sup>*</sup>: indicates equal contribution.)

<table width="100%">

<!--  Where2Explore -->
<tr>
<td width="30%" valign="top"><p><img src="papers/where2explore.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
      Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects
    </h3>
    <a href="https://tritiumr.github.io/" target="_blank">Chuanruo Ning</a>,
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a>,
    Haoran Lu,
    <strong>Kaichun Mo</strong> and 
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a><br>
    arXiv:2309.07473 [cs.RO]<br>
    <p>
       We introduce an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. 
       Our framework explicitly estimates the geometric similarity across different categories, identifying local areas that differ from shapes in the training categories for efficient exploration while concurrently transferring affordance knowledge to similar parts of the objects.
       Extensive experiments in simulated and real-world environments demonstrate our framework's capacity for efficient few-shot exploration and generalization.
    </p>
    <a href="https://arxiv.org/abs/2309.07473" target="_blank">[Paper]</a>
    <a href="bibtex/where2explore.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

 

<!--  Joint Assembly -->
<tr>
<td width="30%" valign="top"><p><img src="papers/jointAssembly.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
      Category-Level Multi-Part Multi-Joint 3D Shape Assembly
    </h3>
    <a href="https://antheali.github.io/" target="_blank">Yichen Li</a>,
    <strong>Kaichun Mo</strong>,
    <a href="https://duanyueqi.github.io/" target="_blank">Yueqi Duan</a>,
    <a href="https://hughw19.github.io/" target="_blank">He Wang</a>, 
    <a href="https://jiequanz.github.io/" target="_blank">Jiequan Zhang</a>,
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a>,
    <a href="https://cdfg.csail.mit.edu/wojciech" target="_blank">Wojciech Matusik</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    arXiv:2303.06163 [cs.CV]<br>
    <p>
      We consider the task of joint-aware multi-part 3D shape assembly. 
      We propose a hierarchical graph learning approach composed of two levels of graph representation learning. 
      The part graph takes part geometries as input to build the desired shape structure, while the joint-level graph uses part joints information and focuses on matching and aligning joints. 
      Extensive experiments demonstrate that our method outperforms previous methods, achieving better shape structure and higher joint alignment accuracy.
    </p>
    <a href="https://arxiv.org/abs/2303.06163" target="_blank">[Paper]</a>
    <a href="bibtex/jointassembly.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

 

   
<!--  RoboAssembly-->
<tr>
<td width="30%" valign="top"><p><img src="papers/roboAssembly.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        RoboAssembly: Learning Generalizable Furniture Assembly Policy in a Novel Multi-robot Contact-rich Simulation Environment
    </h3>
    Mingxin Yu<sup>*</sup>,
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a><sup>*</sup>,
    Zhehuan Chen,
    <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a>,
    <a href="https://fqnchina.github.io/" target='_blank'>Qingnan Fan</a>,
    <strong>Kaichun Mo</strong> and
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a><br>
    arXiv:2112.10143 [cs.RO]<br>
    <p>
    We develop a robotic assembly simulation environment for furniture assembly, and formulate the part assembly problem into a concrete reinforcement learning problem along with a learning pipeline to assemble a diverse set of chairs.
    Experiments show that when testing with unseen chairs, our approach achieves 74.5% success rate on our object-centric setting (without robots) and 50% success rate on our normal setting (with robots).
    </p>
    <a href="https://arxiv.org/abs/2112.10143" target="_blank">[Paper]</a>
    <a href="https://sites.google.com/view/roboticassembly" target="_blank">[Project]</a>
    <a href="bibtex/roboassembly.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- GenStruct -->
<tr>
<td width="30%" valign="top"><p><img src="papers/genstruct.gif" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Compositionally Generalizable 3D Structure Prediction</h3>
    <a href="http://hansf.me/" target="_blank">Songfang Han</a>,
    <a href="http://cseweb.ucsd.edu/~jigu/" target="_blank">Jiayuan Gu</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://ericyi.github.io/" target="_blank">Li Yi</a>, 
    <a href="https://samhu1989.github.io/" target="_blank">Siyu Hu</a>,
    <a href="http://staff.ustc.edu.cn/~xjchen99/" target="_blank">Xuejin Chen</a> and
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    arXiv:2012.02493 [cs.CV]<br>
    <p>
    We tackle the task of 3D shape structure reconstruction from single images in a zero-shot setting, where we train our model on chairs only and show its generalizability to unseen novel categories (e.g. tables, cabinets) without any new training data.
    We bring in the concept of compositional generalizability and propose a novel framework that factorizes the 3D shape reconstruction problem into proper sub-problems, each of which is tackled by a carefully designed neural sub-module with generalizability guarantee. 
    </p>
    <a href="https://arxiv.org/abs/2012.02493" target="_blank">[Paper]</a>
    <a href="https://github.com/hansongfang/CompNet" target="_blank">[Code]</a>
    <a href="https://www.youtube.com/watch?v=a1Mghtz3erM" target="_blank">[Video]</a>
    <a href="bibtex/genstruct.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- AdobeIndoorNav -->
<tr>
<td width="30%" valign="top"><p><img src="papers/adobeindoornav.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based Real-world Indoor Robot Visual Navigation</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://haoxiang.org/academic-homepage/" target="_blank">Haoxiang Li</a>, 
    <a href="https://research.adobe.com/person/zhe-lin/" target="_blank">Zhe Lin</a> and 
    <a href="https://joonyoung-cv.github.io/" target="_blank">Joon-Young Lee</a><br>
    arXiv:1802.08824 [cs.RO]<br>
    <p>
    We propose an indoor navigation dataset for visual navigation and deep reinforcement learning research. We show that the current mapless DRL-based method suffers from the target generalization and scene generalization issues. We propose methods to improve target generalization.
    </p>
    <a href="https://arxiv.org/abs/1802.08824" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/adobeindoornav/" target="_blank">[Project]</a>
    <a href="bibtex/adobeindoornav.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


</table>
</ul>
</div>
<br>


<a name="publications"></a>
<h3>Publications</h3>
<div class="mainsection">
<ul>

(<sup>*</sup>: indicates equal contribution.)

<h4 style="font-size: 20px;">2023</h4>
<table width="100%">

<!--  STOW -->
<tr>
<td width="30%" valign="top"><p><img src="papers/stow.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
      STOW: Discrete-Frame <u>S</u>egmentation and <u>T</u>racking of Unseen <u>O</u>bjects for <u>W</u>arehouse Picking Robots
    </h3>
    <a href="https://yili.vision/" target="_blank">Yi Li</a>,
    <a href="https://nanami18.github.io/" target="_blank">Muru Zhang</a>,
    <a href="https://markusgrotz.github.io/" target="_blank">Markus Grotz</a>,
    <strong>Kaichun Mo</strong> and
    <a href="https://research.nvidia.com/person/dieter-fox" target="_blank">Dieter Fox</a><br>
    <a href="https://www.corl2023.org/" target="_blank">CoRL 2023</a><br>
    <p>
      Segmentation and tracking of unseen object instances in discrete frames pose a significant challenge in dynamic industrial robotic contexts, such as distribution warehouses. 
      Our task involves working with a discrete set of frames separated by indefinite periods, during which substantial changes to the scene may occur, such like in the cases of object rearrangements, including shifting, removal, and partial occlusion by new items.
      To address these demanding challenges, we introduce new synthetic and real-world datasets that replicate these industrial and household scenarios. 
      Furthermore, we propose a novel paradigm for joint segmentation and tracking in discrete frames, alongside a transformer module that facilitates efficient inter-frame communication. 
      Our approach significantly outperforms recent methods in our experiments.
    </p>
    <a href="https://openreview.net/pdf?id=48qUHKUEdBf" target="_blank">[Paper]</a>
    <a href="https://sites.google.com/view/stow-corl23" target="_blank">[Project]</a>
    <a href="bibtex/stow.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

 
<!--  Copilot -->
<tr>
<td width="30%" valign="top"><p><img src="papers/copilot.jpg" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        COPILOT: Human Collision Prediction and Localization from Multi-view Egocentric Videos
    </h3>
    <a href="https://cs.stanford.edu/~bxpan/" target="_blank">Boxiao Pan</a>,
    <a href="https://cs.stanford.edu/people/bshen88/" target="_blank">Bokui Shen</a><sup>*</sup>,
    <a href="https://davrempe.github.io/" target="_blank">Davis Rempe</a><sup>*</sup>,
    <a href="https://paschalidoud.github.io/" target="_blank">Despoina Paschalidou</a>,
    <strong>Kaichun Mo</strong>,
    <a href="https://yanchaoyang.github.io/" target="_blank">Yanchao Yang</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="http://iccv2023.thecvf.com/" target="_blank">ICCV 2023</a><br>
    <p>
    We propose the problem of predicting human-scene collisions from multi-view egocentric RGB videos captured from an exoskeleton.
    Specifically, the problem consists of predicting: (1) if a collision will happen in the next H seconds; (2) which body joints might be involved in a collision; and (3) where in the scene might cause the collision, in the form of a spatial heatmap.
    To solve this problem, we present COPILOT, a COllision PredIction and LOcalization Transformer that tackles all three sub-tasks in a multi-task setting, effectively leveraging multi-view video inputs through a proposed 4D attention operation across space, time, and viewpoint.   </p>
    <a href="https://arxiv.org/abs/2210.01781" target="_blank">[Paper]</a>
    <a href="https://sites.google.com/stanford.edu/copilot" target="_blank">[Project]</a>
    <a href="bibtex/copilot.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

 
<!--  Geometric Concept -->
<tr>
<td width="30%" valign="top"><p><img src="papers/GeoConcept.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
      Toward Learning Geometric Eigen-Lengths Crucial for Fitting Tasks
    </h3>
    <a href="https://yijiaweng.github.io/" target="_blank">Yijia Weng</a>,
    <strong>Kaichun Mo</strong>,
    <a href="https://scholar.google.com/citations?user=Z7zLvdkAAAAJ&hl=en" target="_blank">Ruoxi Shi</a>,
    <a href="https://yanchaoyang.github.io/" target="_blank">Yanchao Yang</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://icml.cc/Conferences/2023" target="_blank">ICML 2023</a>
    <p>
      Some extremely low-dimensional yet crucial geometric eigen-lengths often determine the success of some geometric tasks. 
      For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway.
      In this work, we propose a novel problem of discovering key geometric concepts (e.g., height, width, radius) of objects for robotic fitting tasks.
      We explore potential solutions and demonstrate the feasibility of learning eigen-lengths from simply observing successful and failed fitting trials.
    We also attempt geometric grounding for more accurate eigen-length measurement and study the reusability of the learned geometric eigen-lengths across multiple tasks.
    </p>
    <a href="https://yijiaweng.github.io/geo-eigen-length/" target="_blank">[Project]</a>
    <a href="bibtex/GeoConcept.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!--  JacobiNeRF -->
<tr>
<td width="30%" valign="top"><p><img src="papers/JacobiNeRF.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
      JacobiNeRF: NeRF Shaping with Mutual Information Gradients
    </h3>
    <a href="https://xxm19.github.io/" target="_blank">Xiaomeng Xu</a><sup>*</sup>,
    <a href="https://yanchaoyang.github.io/" target="_blank">Yanchao Yang</a><sup>*</sup>,
    <strong>Kaichun Mo</strong>,
    <a href="https://cs.stanford.edu/~bxpan/" target="_blank">Boxiao Pan</a>,
    <a href="https://ericyi.github.io/" target="_blank">Li Yi</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a>
    <p>
      We propose a method that trains a neural radiance field (NeRF) to encode not only the appearance of the scene but also mutual correlations between scene points, regions, or entities – aiming to capture their co-variation patterns.
      In contrast to the traditional first-order photometric reconstruction objective, our method explicitly regularizes the learning dynamics to align the Jacobians of highlycorrelated entities, which proves to maximize the mutual information between them under random scene perturbations.
      Experiments show that JacobiNeRF is more efficient in propagating annotations among 2D pixels and 3D points compared to NeRFs without mutual information shaping, especially in extremely sparse label regimes – thus reducing annotation burden.
    </p>
    <a href="https://arxiv.org/abs/2304.00341" target="_blank">[Paper]</a>
    <a href="https://github.com/xxm19/jacobinerf" target="_blank">[Code]</a>
    <a href="resources/jacobinerf_poster.png" target="_blank">[Poster]</a>
    <a href="resources/jacobinerf_slides.pdf" target="_blank">[Slides]</a>
    <a href="bibtex/JacobiNeRF.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!--  DualAfford -->
<tr>
<td width="30%" valign="top"><p><img src="papers/dualAfford.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation
    </h3>
    <a href="https://sxy7147.github.io/" target="_blank">Yan Zhao</a><sup>*</sup>,
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a><sup>*</sup>,
    Zhehuan Chen,
    <a href="https://www.linkedin.com/in/yourong-zhang-2b1aab23a/" target="_blank">Yourong Zhang</a>,
    <a href="https://fqnchina.github.io/" target='_blank'>Qingnan Fan</a>,
    <strong>Kaichun Mo</strong> and
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a><br>
    <a href="https://iclr.cc/Conferences/2023" target="_blank">ICLR 2023</a>
    <p>
    We propose a novel learning framework, DualAfford, to learn collaborative affordance for dual-gripper manipulation tasks. 
    The core design of the approach is to reduce the quadratic problem for two grippers into two disentangled yet interconnected subtasks for efficient learning.
    Using the large-scale PartNet-Mobility and ShapeNet datasets, we set up four benchmark tasks for dual-gripper manipulation. 
    </p>
    <a href="https://arxiv.org/abs/2207.01971" target="_blank">[Paper]</a>
    <a href="https://hyperplane-lab.github.io/DualAfford/" target="_blank">[Project]</a>
    <a href="bibtex/dualAfford.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- SceneHGN -->
<tr>
<td width="30%" valign="top"><p><img src="papers/scenehgn.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry
    </h3>
    <a href="http://geometrylearning.com/lin/" target="_blank">Lin Gao</a>,
    Jia-Mu Sun,
    <strong>Kaichun Mo</strong>, 
    <a href="http://users.cs.cf.ac.uk/Yukun.Lai/" target="_blank">Yu-Kun Lai</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="http://people.geometrylearning.com/~jieyang/" target="_blank">Jie Yang</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2023</a><br>
    <p>
    We propose a hierarchical graph network for 3D indoor scenes that takes into account the full hierarchy from the room level to the object level, then finally to the object part level. 
    Therefore for the first time, our method is able to directly generate plausible 3D room content, including furniture objects with fine-grained geometry, and their layout. 
    Our generation network is a conditional recursive neural network (RvNN) based variational autoencoder (VAE) that learns to generate detailed content with fine-grained geometry for a room, given the room boundary as the condition. 
    Extensive experiments demonstrate that our method produces superior generation results.
    We also demonstrate that our method is effective for various applications such as part-level room editing, room interpolation, and room generation by arbitrary room boundaries. 
    </p>
    <a href="https://ieeexplore.ieee.org/abstract/document/10018465" target="_blank">[Paper]</a>
    <a href="http://geometrylearning.com/scenehgn/" target="_blank">[Project]</a>
    <a href="bibtex/scenehgn.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>
 
<tr><td><br/></td></tr>

<!-- Seg&Struct -->
<tr>
<td width="30%" valign="top"><p><img src="papers/segstruct.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        Seg&#38;Struct: The Interplay between Part Segmentation and Structure Inference for 3D Shape Parsing
    </h3>
    <a href="https://sites.google.com/view/jeonghyunkeem/" target="_blank">Jeonghyun Kim</a>,
    <strong>Kaichun Mo</strong>, 
    <a href="https://mhsung.github.io/" target="_blank">Minhyuk Sung</a><sup>*</sup> and
    <a href="http://uvrlab.org/professor" target="_blank">Woontack Woo</a><sup>*</sup><br>
    <a href="https://wacv2023.thecvf.com/" target="_blank">WACV 2023</a>
    <p>
    We propose Seg&#38 Struct, a supervised learning framework leveraging the interplay between part segmentation and structure inference and demonstrating their synergy in an integrated framework. 
    Our framework first decomposes a raw input shape into part segments using off-the-shelf algorithm, whose outputs are then mapped to nodes in a part hierarchy, establishing point-to-part associations. 
    Following this, ours predicts the structural information, e.g., part bounding boxes the part relationships. 
    Lastly, the segmentation is rectified by examining the confusion of part boundaries using the structure-based part features.
    </p>
    <a href="https://arxiv.org/abs/2211.00382" target="_blank">[Paper]</a>
    <a href="https://sites.google.com/view/segstruct" target="_blank">[Project]</a>
    <a href="bibtex/segstruct.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
</table>

<h4 style="font-size: 20px;">2022</h4>
<table width="100%">

   
<!-- GIMO -->
<tr>
<td width="30%" valign="top"><p><img src="papers/gimo.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        GIMO: Gaze-Informed Human Motion Prediction in Context
    </h3>
    <a href="https://y-zheng18.github.io/" target="_blank">Yang Zheng</a>,
    <a href="https://yanchaoyang.github.io/" target="_blank">Yanchao Yang</a>,
    <strong>Kaichun Mo</strong>, 
    <a href="http://lijiaman.github.io/" target="_blank">Jiaman Li</a>,
    <a href="https://ytrock.com/" target="_blank">Tao Yu</a>,
    <a href="https://liuyebin.com/" target="_blank">Yebin Liu</a>,
    <a href="https://ckllab.stanford.edu/c-karen-liu" target="_blank">Karen Liu</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>
    <p>
    We propose a large-scale human motion dataset that delivers high-quality body pose sequences, scene scans, as well as ego-centric views with eye gaze that serves as a surrogate for inferring human intent.
    After adapting and benchmarking existing state-of-the-art methods, we introduce a novel network architecture that enables bidirectional communication between the gaze and motion branches for better ego-centric human motion predictions.
    </p>
    <a href="https://arxiv.org/abs/2204.09443" target="_blank">[Paper]</a>
    <a href="https://geometry.stanford.edu/projects/gimo/" target="_blank">[Project]</a>
    <a href="bibtex/gimo.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- AdaAfford -->
<tr>
<td width="30%" valign="top"><p><img src="papers/adaafford.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions
    </h3>
    <a href="https://github.com/galaxy-qazzz" target="_blank">Yian Wang</a><sup>*</sup>,
    <a href="https://warshallrho.github.io" target="_blank">Ruihai Wu</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    Jiaqi Ke,
    <a href="https://fqnchina.github.io/" target='_blank'>Qingnan Fan</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a><br>
    <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>
    <p>
    Taking only passive observations as inputs, Where2Act ignores many hidden but important kinematic constraints (e.g., joint location and limits) and dynamic factors (e.g., joint friction and restitution), therefore losing significant accuracy for test cases with such uncertainties. In this paper, we propose a novel framework, named AdaAfford, that learns to perform very few test-time interactions for quickly adapting the affordance priors to more accurate instance-specific posteriors.
    </p>
    <a href="https://arxiv.org/abs/2112.00246" target="_blank">[Paper]</a>
    <a href="https://hyperplane-lab.github.io/AdaAfford/" target="_blank">[Project]</a>
    <a href="bibtex/adaafford.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>



<!-- Learn2Fix -->
<tr>
<td width="30%" valign="top"><p><img src="papers/learn2fix.gif" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction
    </h3>
    <a href="https://evelinehong.github.io/" target="_blank">Yining Hong</a>,
    <strong>Kaichun Mo</strong>, 
    <a href="https://ericyi.github.io/" target="_blank">Li Yi</a>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a>,
    <a href="https://web.mit.edu/torralba/www/" target="_blank">Antonio Torralba</a>,
    <a href="http://web.mit.edu/cocosci/josh.html" target="_blank">Joshua Tenenbaum</a> and
    <a href="https://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a><br>
    <a href="https://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a>
    <p>
    This paper studies the problem of fixing malfunctional 3D objects.
    Given a malfunctional object, humans can perform mental simulations to reason about its functionality and figure out how to fix it.
    We propose FixIt, a dataset that contains around 5k poorly-designed 3D physical objects paired with choices to fix them.
    We present FixNet, a novel framework that seamlessly incorporates perception and physical dynamics.
    Specifically, FixNet consists of a perception module to extract the structured representation from the 3D point cloud, a physical dynamics prediction module to simulate the results of interactions on 3D objects, and a functionality prediction module to evaluate the functionality and choose the correct fix. 
    </p>
    <a href="https://arxiv.org/abs/2205.02834" target="_blank">[Paper]</a>
    <a href="http://fixing-malfunctional.csail.mit.edu/" target="_blank">[Project]</a>
    <a href="https://www.youtube.com/watch?v=-dLJxewGqf4" target="_blank">[Video]</a>
    <a href="bibtex/learn2fix.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>



<!-- IFR-Explore -->
<tr>
<td width="30%" valign="top"><p><img src="papers/ifrexplore.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes
    </h3>
    <a href="https://scholar.google.com/citations?hl=en&user=sz989MIAAAAJ" target="_blank">Qi Li</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="https://yanchaoyang.github.io/" target="_blank">Yanchao Yang</a>,
    <a href="https://hangzhaomit.github.io/" target="_blank">Hang Zhao</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a>
    <p>
    We take the first step in building AI system learning inter-object functional relationships in 3D indoor environments (e.g., a switch on the wall turns on or off the light, a remote control operates the TV).
    The key technical contributions are modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes. 
    </p>
    <a href="https://arxiv.org/abs/2112.05298" target="_blank">[Paper]</a>
    <a href="https://github.com/liqi17thu/IFR_explore" target="_blank">[Code]</a>
    <a href="bibtex/ifrexplore.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- Object-Pursuit -->
<tr>
<td width="30%" valign="top"><p><img src="papers/objectpursuit.png" width="240" alt="" style="border-style: none; margin-left: 30; margin-right: auto;" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        Object Pursuit: Building a Space of Objects via Discriminative Weight Generation
    </h3>
    <a href="https://scholar.google.com/citations?user=wNKoPGAAAAAJ" target="_blank">Chuanyu Pan</a><sup>*</sup>,
    <a href="https://yanchaoyang.github.io/" target="_blank">Yanchao Yang</a><sup>*</sup>,
    <strong>Kaichun Mo</strong>,
    <a href="https://scholar.google.com/citations?user=qDseo3cAAAAJ" target="_blank">Yueqi Duan</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a>
    <p>
    We propose a framework to continuously learn object-centric representations for visual learning and
    understanding.
    Our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations.
    Throughout learning, objects are streamed one by one in random order with unknown identities, and
    are associated with latent codes that can synthesize discriminative weights for each object through
    a convolutional hypernetwork.
    </p>
    <a href="https://arxiv.org/abs/2112.07954" target="_blank">[Paper]</a>
    <a href="https://github.com/pptrick/Object-Pursuit" target="_blank">[Code]</a>
    <a href="bibtex/objectpursuit.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
    
   
<!-- VAT-Mart -->
<tr>
<td width="30%" valign="top"><p><img src="papers/vat_mart.gif" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects
    </h3>
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a><sup>*</sup>,
    <a href="https://www.researchgate.net/profile/Yan-Zhao-182" target="_blank">Yan Zhao</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="https://guozz.cn/" target='_blank'>Zizheng Guo</a>,
    <a href="https://github.com/galaxy-qazzz" target='_blank'>Yian Wang</a>,
    <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a>,
    <a href="https://fqnchina.github.io/" target='_blank'>Qingnan Fan</a>,
    <a href="https://xuelin-chen.github.io/" target='_blank'>Xuelin Chen</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a><br>
    <a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a>
    <p>
    We propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. 
    We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes.
    </p>
    <a href="https://arxiv.org/abs/2106.14440" target="_blank">[Paper]</a>
    <a href="https://hyperplane-lab.github.io/vat-mart" target="_blank">[Project]</a>
    <a href="https://www.youtube.com/watch?v=HjhsLKf1eQY" target="_blank">[Video]</a>
    <a href="bibtex/vat_mart.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- DSG-Net -->
<tr>
<td width="30%" valign="top"><p><img src="papers/dsgnet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation</h3>
    <a href="http://people.geometrylearning.com/~jieyang/" target="_blank">Jie Yang</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="http://users.cs.cf.ac.uk/Yukun.Lai/" target="_blank">Yu-Kun Lai</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="http://geometrylearning.com/lin/" target="_blank">Lin Gao</a><br>
    <a href="https://dl.acm.org/journal/tog" target="_blank">ACM Transactions on Graphics (ToG) 2022</a>, <i>to be presented at
        <a href="https://s2022.siggraph.org/" target="_blank">SIGGRAPH 2022</a></i><br>
    <p>
    We introduce DSG-Net, a deep neural network that learns a disentangled structured mesh representation for 3D shapes, where two key aspects of shapes, geometry and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with intuitive control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged.
    Our method not only supports controllable generation applications, but also produces high-quality synthesized shapes.
    </p>
    <a href="https://arxiv.org/abs/2008.05440" target="_blank">[Paper]</a>
    <a href="http://geometrylearning.com/dsg-net/" target="_blank">[Project]</a>
    <a href="https://youtu.be/UJH0dITlfsw" target="_blank">[Video]</a>
    <a href="bibtex/dsgnet.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
</table>

<h4 style="font-size: 20px;">2021</h4>
<table width="100%">

<!-- O2O-Aford -->
<tr>
<td width="30%" valign="top"><p><img src="papers/o2oafford.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning
    </h3>
    <strong>Kaichun Mo</strong>, 
    <a href="https://yzqin.github.io/" target="_blank">Yuzhe Qin</a>, 
    <a href="https://www.fbxiang.com/" target="_blank">Fanbo Xiang</a>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://www.robot-learning.org/home" target="_blank">CoRL 2021</a>
    <p>
    Contrary to the vast literature in modeling, perceiving, and understanding agent-object interaction in computer vision and robotics, very few past works have studied the task of object-object interaction, which also plays an important role in robotic manipulation and planning tasks. 
    There is a rich space of object-object interaction scenarios in our daily life, such as placing an object on a messy tabletop, fitting an object inside a drawer, pushing an object using a tool, etc. 
    In this paper, we propose a large-scale object-object affordance learning framework that requires no human annotation or demonstration.
    </p>
    <a href="https://arxiv.org/abs/2106.15087" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/o2oafford" target="_blank">[Project]</a>
    <a href="https://www.youtube.com/watch?v=cbDSalrMhlo" target="_blank">[Video]</a>
    <a href="bibtex/o2oafford.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- Regrasp -->
<tr>
<td width="30%" valign="top"><p><img src="papers/regrasp.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>
        Learning to Regrasp by Learning to Place
    </h3>
    <a href="https://www.linkedin.com/in/shuo-cheng-1bb289155/" target="_blank">Shuo Cheng</a>,
    <strong>Kaichun Mo</strong> and
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a><br>
    <a href="https://www.robot-learning.org/home" target="_blank">CoRL 2021</a>
    <p>
    Regrasping is needed whenever a robot's current grasp pose fails to perform desired manipulation tasks.
    In this paper, we propose a system for robots to take partial point clouds of an object and the supporting environment as inputs and output a sequence of pick-and-place operations to transform an initial object grasp pose to the desired object grasp poses. The key technique includes a neural stable placement predictor and a regrasp graph based solution through leveraging and changing surrounding environment.
    </p>
    <a href="https://arxiv.org/abs/2109.08817" target="_blank">[Paper]</a>
    <a href="https://sites.google.com/view/regrasp" target="_blank">[Project]</a>
    <a href="bibtex/regrasp.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

    
<!-- Where2Act -->
<tr>
<td width="30%" valign="top"><p><img src="papers/where2act.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Where2Act: From Pixels to Actions for Articulated 3D Objects</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a>,
    <a href="http://www.mustafamukadam.com/" target='_blank'>Mustafa Mukadam</a>,
    <a href="https://www.cs.cmu.edu/~abhinavg/" target='_blank'>Abhinav Gupta</a> and
    <a href="https://shubhtuls.github.io/" target='_blank'>Shubham Tulsiani</a><br>
    <a href="http://iccv2021.thecvf.com/" target="_blank">ICCV 2021</a><br>
    <i><b style="color: red;">Featured in: </b>
        <a href="https://venturebeat.com/business/stanford-researchers-propose-ai-that-figures-out-how-to-use-real-world-objects/" target="_blank">VentureBeat</a>, 
        <a href="https://ai.googleblog.com/2022/06/scanned-objects-by-google-research.html" target="_blank">Google AI Blog</a>
    </i>
    <p>
    One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. 
    In this paper, we take a step towards that long-term goal -- we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. 
    We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. 
    </p>
    <a href="https://arxiv.org/abs/2101.02692" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/where2act" target="_blank">[Project]</a>
    <a href="https://www.youtube.com/watch?v=cdMSZru3Aa8" target="_blank">[Video]</a>
    <a href="bibtex/where2act.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

    
<!-- Rethink PC-GAN Sampling -->
<tr>
<td width="30%" valign="top"><p><img src="papers/rethinkpcgan.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</h3>
    <a href="http://ai.stanford.edu/~hewang/" target="_blank">He Wang</a><sup>*</sup>, 
    <a href="https://github.com/JzMaple" target="_blank">Zetian Jiang</a><sup>*</sup>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://learn3dg.github.io/" target="_blank">CVPR 2021 Workshop "Learning to generate 3D Shapes and Scenes"</a><br>
    <p>
    We show that sampling-insensitive discriminators (e.g.PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g.PointNet++, DGCNN) fail to guide valid shape generation. 
    We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. 
    We point out that, though recent research has been focused on the generator design, the main bottleneck of point cloud GAN actually lies in the discriminator design.
    </p>
    <a href="https://arxiv.org/abs/2006.07029" target="_blank">[Paper]</a>
    <a href="bibtex/rethinkpcgan.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
</table>

<h4 style="font-size: 20px;">2020</h4>
<table width="100%">


<!-- GenPartAss -->
<tr>
<td width="30%" valign="top"><p><img src="papers/genpartass.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Generative 3D Part Assembly via Dynamic Graph Learning</h3>
    <a href="https://jialeihuang.github.io/" target="_blank">Jialei Huang</a><sup>*</sup>, 
    <a href="https://championchess.github.io/" target="_blank">Guanqi Zhan</a><sup>*</sup>, 
    <a href="https://fqnchina.github.io/" target="_blank">Qingnan Fan</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/" target="_blank">Baoquan Chen</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><br>
    <a href="https://nips.cc/Conferences/2020/" target="_blank">NeurIPS 2020</a><br>
    <p>
    Analogous to buying an IKEA furniture, given a set of 3D part point clouds, we predict 6-Dof part poses to assemble a 3D shape.
    To tackle this problem, we propose an assembly-oriented dynamic graph learning framework that leverages an iterative graph neural network as a backbone. 
    It explicitly conducts sequential part assembly refinements in a coarse-to-fine manner, exploits a pair of part relation reasoning module and part aggregation module for dynamically adjusting both part features and their relations in the part graph.
    </p>
    <a href="https://arxiv.org/abs/2006.07793" target="_blank">[Paper]</a>
    <a href="https://hyperplane-lab.github.io/Generative-3D-Part-Assembly/" target="_blank">[Project]</a>
    <a href="bibtex/genpartass.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- ImPartAss -->
<tr>
<td width="30%" valign="top"><p><img src="papers/impartass.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Learning 3D Part Assembly from a Single Image</h3>
    <a href="https://antheali.github.io/" target="_blank">Yichen Li</a><sup>*</sup>,
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="https://linsats.github.io/" target="_blank">Lin Shao</a>,
    <a href="https://mhsung.github.io/" target="_blank">Minhyuk Sung</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a><br>
    <i>(Also be present at <a href="https://holistic-3d.github.io/eccv20/" target="_blank">Holistic Scene Structures for 3D Vision</a>)</i><br>
    <p>
    We introduce a novel problem, single-image-guided 3D part assembly, that assembles 3D shapes from parts given a complete set of part point cloud scans and a single 2D image depicting the object.
    The task is motivated by the robotic assembly setting and the estimated per-part poses serve as a vision-based initialization before robotic planning and control components.
    </p>
    <a href="https://arxiv.org/abs/2003.09754" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~liyichen/projects/assembly/" target="_blank">[Project]</a>
    <a href="https://youtu.be/TCME39wusek" target="_blank">[Video (1-min)]</a>
    <a href="https://youtu.be/gtaBaEAs22s" target="_blank">[Video (7-min)]</a>
    <a href="bibtex/impartass.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

    
<!-- PT2PC -->
<tr>
<td width="30%" valign="top"><p><img src="papers/pt2pc.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://ai.stanford.edu/~hewang/" target="_blank">He Wang</a>,
    <a href="https://sites.google.com/site/skywalkeryxc/" target="_blank">Xinchen Yan</a> and
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>
    <p>
    This paper investigates the novel problem of generating 3D shape point cloud geometry from a symbolic part tree representation. 
    In order to learn such a conditional shape generation procedure in an end-to-end fashion, we propose a conditional GAN "part tree"-to-"point cloud" model (PT2PC) that disentangles the structural and geometric factors. 
    </p>
    <a href="https://arxiv.org/abs/2003.08624" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/pt2pc/" target="_blank">[Project]</a>
    <a href="https://youtu.be/GZGxaFx-kgw" target="_blank">[Video (1-min)]</a>
    <a href="https://youtu.be/LYVdoTWfy5I" target="_blank">[Video (10-min)]</a>
    <a href="bibtex/pt2pc.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- SAPIEN -->
<tr>
<td width="30%" valign="top"><p><img src="papers/sapien.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>SAPIEN: A SimulAted Part-based Interactive ENvironment</h3>
    <a href="https://www.fbxiang.com/" target="_blank">Fanbo Xiang</a>, 
    <a href="https://yzqin.github.io/" target="_blank">Yuzhe Qin</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://www.linkedin.com/in/yikuan-xia-4418a9170/" target="_blank">Yikuan Xia</a>, 
    <a href="https://berniezhu.github.io/" target="_blank">Hao Zhu</a>, 
    <a href="https://fangchenliu.github.io/" target="_blank">Fangchen Liu</a>, 
    <a href="http://cseweb.ucsd.edu/~mil070/" target="_blank">Minghua Liu</a>, 
    <a href="https://jianghanxiao.github.io/" target="_blank">Hanxiao Jiang</a>, 
    Yifu Yuan</a>, 
    <a href="http://ai.stanford.edu/~hewang/" target="_blank">He Wang</a>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="https://angelxuanchang.github.io/" target="_blank">Angel X.Chang</a>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a>, <i style="color: red">Oral Presentation</i>
    <p>
    We propose a realistic and physics-rich simulation environment hosting large-scale 3D articulated objects from ShapeNet and PartNet. 
    Our PartNet-Mobility dataset contains 14,068 articulated parts with part motion information for 2,346 object models from 46 common indoor object categories.
    SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.
    </p>
    <a href="https://arxiv.org/abs/2003.08515" target="_blank">[Paper]</a>
    <a href="http://sapien.ucsd.edu/publication" target="_blank">[Project Page]</a>
    <a href="https://github.com/haosulab/SAPIEN-Release" target="_blank">[Code]</a>
    <a href="https://youtu.be/K2yOeJhJXzM" target="_blank">[Demo]</a>
    <a href="https://youtu.be/WnTxEeSHebk" target="_blank">[Video]</a>
    <a href="bibtex/sapien.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
    
<!-- StructEdit -->
<tr>
<td width="30%" valign="top"><p><img src="papers/structedit.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>StructEdit: Learning Structural Shape Variations</h3>
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="http://paulguerrero.net/" target="_blank">Paul Guerrero</a><sup>*</sup>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a>, 
    <a href="http://peterwonka.net/" target="_blank">Peter Wonka</a>, 
    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a> and 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a><br>
    <i><b style="color: red;">Featured in: </b>
        <a href="https://www.rsipvision.com/CVPR2020-Tuesday/26/" target="_blank">CVPR Daily (Tue)</a>
    </i><br>
    <i><b style="color: red;">Video: </b>
        <a href="https://www.youtube.com/watch?v=Er2sA-lfsI8" target="_blank">CVPR Workshop: Learning 3D Generative Models (Invited Talk By Paul Guerrero)</a>
    </i><br>
    <p>
    We learn local shape edits (shape deltas) space that captures both discrete structural changes and continuous variations.
    Our approach is based on a conditional variational autoencoder (cVAE) for encoding and decoding shape deltas, conditioned on a source shape.
    The learned shape delta spaces support shape edit suggestions, shape analogy, and shape edit transfer, much better than StructureNet, on the PartNet dataset.
    </p>
    <a href="https://arxiv.org/abs/1911.11098" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/structedit/" target="_blank">[Project]</a>
    <a href="https://youtu.be/OrUzdkRUoE0" target="_blank">[Video]</a>
    <a href="bibtex/structedit.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>

<!-- LearningToGroup -->
<tr>
<td width="30%" valign="top"><p><img src="papers/learningtogroup.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories</h3>
    <a href="https://tiangeluo.github.io/" target="_blank">Tiange Luo</a>, 
    <strong>Kaichun Mo</strong>, 
    <a href="https://sites.google.com/view/zhiao-huang" target="_blank">Zhiao Huang</a>, 
    <a href="http://jerryxu.net/" target="_blank">Jiarui Xu</a>,
    <a href="https://samhu1989.github.io/" target="_blank">Siyu Hu</a>, 
    <a href="https://scholar.google.com/citations?user=VZHxoh8AAAAJ" target="_blank">Liwei Wang</a> and 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    <a href="https://iclr.cc/Conferences/2020" target="_blank">ICLR 2020</a><br>
    <p>
    We address the problem of learning to discover 3D parts for objects in unseen categories under the zero-shot learning setting.
    We propose a learning-based iterative grouping framework which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion.
    On PartNet, we demonstrate that our method can transfer knowledge of parts learned from 3 training categories to 21 unseen categories.
    </p>
    <a href="https://arxiv.org/abs/2002.06478" target="_blank">[Paper]</a>
    <a href="https://tiangeluo.github.io/projectpages/ltg.html" target="_blank">[Project]</a>
    <a href="https://iclr.cc/virtual_2020/poster_rkl8dlHYvB.html" target="_blank">[Video]</a>
    <a href="bibtex/learningtogroup.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
</table>

<h4 style="font-size: 20px;">2019</h4>
<table width="100%">

<!-- StructureNet -->
<tr>
<td width="30%" valign="top"><p><img src="papers/structurenet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>StructureNet: Hierarchical Graph Networks for 3D Shape Generation</h3>
    <strong>Kaichun Mo</strong><sup>*</sup>, 
    <a href="http://paulguerrero.net/" target="_blank">Paul Guerrero</a><sup>*</sup>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a>, 
    <a href="http://peterwonka.net/" target="_blank">Peter Wonka</a>, 
    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a> and 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="https://sa2019.siggraph.org/" target="_blank">ACM Transactions on Graphics (SIGGRAPH Asia 2019)</a><br>
    <p>
    We introduce a hierarchical graph network for learning structure-aware shape generation which (i) can directly encode shape parts represented as such n-ary graphs; (ii) can be robustly trained on large and complex shape families such as PartNet; and (iii) can be used to generate a great diversity of realistic structured shape geometries with both both continuous geometric and discrete structural variations.
    </p>
    <a href="https://arxiv.org/abs/1908.00575" target="_blank">[Paper]</a>
    <a href="https://cs.stanford.edu/~kaichun/structurenet/" target="_blank">[Project]</a>
    <a href="bibtex/structurenet.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- PartNet -->
<tr>
<td width="30%" valign="top"><p><img src="papers/partnet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</h3>
    <strong>Kaichun Mo</strong>, 
    <a href="http://cseweb.ucsd.edu/~shz338/" target="_blank">Shilin Zhu</a>, 
    <a href="https://angelxuanchang.github.io/" target="_blank">Angel X.Chang</a>, 
    <a href="https://cs.stanford.edu/~ericyi/" target="_blank">Li Yi</a>, 
    <a href="https://subarnatripathi.github.io/" target="_blank">Subarna Tripathi</a>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a> and 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><br>
    <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a><br>
    <i>(Also be present at <a href="https://sites.google.com/view/fgvc6/home" target="_blank">The Sixth Workshop on Fine-Grained Visual Categorization</a> and <a href="https://3dscenegen.github.io/" target="_blank">the 3D Scene Generation Workshop</a>)</i><br>
    <i><b style="color: red;">Featured in: </b>
        <a href="https://www.therobotreport.com/intel-osu-uc-san-diego-reinforcement-learning-partnet-robots/" target="_blank">The Robot Report</a>,
        <a href="https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/partnet-helps-robots-understand-what-things-are" target="_blank">IEEE Spectrum</a>,
        <a href="https://www.roboticsbusinessreview.com/ai/researchers-launch-26k-object-dataset-to-help-robots-learn-shapes/" target="_blank">Robotics Business Review</a>, 
        <a href="https://techcrunch.com/2019/06/17/intel-is-doing-the-hard-work-necessary-to-make-sure-robots-can-operate-your-microwave/" target="_blank">TechCrunch</a>, 
        <a href="https://venturebeat.com/2019/06/17/intel-highlights-ai-that-can-see-around-corners-coach-children-on-the-autism-spectrum-and-more-during-cvpr/" target="_blank">VentureBeat</a>, 
        <a href="https://www.intel.ai/introducing-partnet/" target="_blank">Intel AI Blog</a>, 
        <a href="https://newsroom.intel.com/news/intel-offers-computer-powered-echolocation-tech-artificial-intelligence-research-cvpr-2019" target="_blank">Intel Newsroom</a>
    </i><br>
    <p>
    We propose a 3D object database with fine-grained and hierarchical part annotation, to assist segmentation and affordance research. We benchmark three part-level object understanding tasks: fine-grained semantic segmentation, hierarchical semantic segmentation and instance segmentation. We also propose a novel method for instance segmentation.
    </p>
    <a href="https://arxiv.org/abs/1812.02713" target="_blank">[Paper]</a>
    <a href="https://partnet.cs.stanford.edu" target="_blank">[Project]</a>
    <a href="bibtex/partnet.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>
</table>

<h4 style="font-size: 20px;">2018 and before</h4>
<table width="100%">

<!-- PointNet -->
<tr>
<td width="30%" valign="top"><p><img src="papers/pointnet.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</h3>
    <a href="https://web.stanford.edu/~rqi/" target="_blank">Charles R. Qi</a><sup>*</sup>, 
    <a href="http://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><sup>*</sup>, 
    <strong>Kaichun Mo</strong> and 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas J. Guibas</a><br>
    <a href="http://cvpr2017.thecvf.com/" target="_blank">CVPR 2017</a>, <i style="color: red">Oral Presentation</i><br>
    <p>
    We propose novel neural networks to directly consume an unordered point cloud as input, without converting to other 3D representations such as voxel grids first. Rich theoretical and empirical analyses are provided.
    </p>
    <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank">[Paper]</a>
    <a href="http://stanford.edu/~rqi/pointnet/" target="_blank">[Project]</a>
    <a href="bibtex/pointnet.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

<tr><td><br/></td></tr>


<!-- AcceleKaczmarz -->
<tr>
<td width="30%" valign="top"><p><img src="papers/kaczmarz.png" width="300" alt="" style="border-style: none" align="top"></p></td>
<td width="70%" valign="top"><p>
    <h3>Accelerating Random Kaczmarz Algorithm Based on Clustering Information</h3>
    <a href="http://bcmi.sjtu.edu.cn/~liyujun/" target="_blank">Yujun Li</a>, 
    <strong>Kaichun Mo</strong> and 
    <a href="https://dblp.org/pers/hd/y/Ye:Haishan" target="_blank">Haishan Ye</a><br>
    <a href="https://www.aaai.org/Conferences/AAAI/aaai16.php" target="_blank">AAAI 2016</a><br>
    <p>
    Using the property of randomly sampled data in high-dimensional space, we propose an accelerated algorithm based on clustering information to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss lemma. Additionally, we theoretically demonstrate convergence improvement on block Kaczmarz algorithm.
    </p>
    <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11938" target="_blank">[Paper]</a>
    <a href="bibtex/kaczmarz.txt" target="_blank">[BibTex]</a>
</p></td>
</tr>

</table>
</ul>
</div>
<br>

<a name="talks"></a>
<h3>Invited Talks</h3>
<div class="section">
<ul>
    <li>[May 2022] Learning Compositional and Actionable Visual Representations for 3D Shapes, <a href="https://sites.google.com/view/visionseminar" target="_blank">MIT Vision and Graphics Seminar</a>.</li>
    <li>[May 2022] Compositional and Structural Learning on 3D Shapes, Meta AI.</li>
    <li>[Mar 2022] Learning Compositional, Structural, and Actionable Visual Representations for 3D Shapes, Google Research.</li>
    <li>[Mar 2022] Learning Actionable and Compositional Visual Representations for 3D Shapes, Univ. of Washington.</li>
    <li>[Feb 2022] Learning Actionable and Compositional Visual Representations for 3D Shapes, at Robotics Lab, NVIDIA.</li>
    <li>[Dec 2021] <a href="https://youtu.be/pReOys_pbvY?t=914" target="_blank">Self-supervised Affordance Learning for Manipulating Articulated 3D Objects</a>, at <a href="http://www.robot-learning.ml/2021/" target="_blank">4th Robot Learning Workshop</a>, NeurIPS 2021.</li>
    <li>[Dec 2021] Learning 3D Shape Structure and Semantics, at CompVision Seminar, UC Berkeley.</li>
    <li>[Nov 2021] <a href="https://www.youtube.com/watch?v=Z3V60LPPTxo" target="_blank">Learning 3D Shape Structure and Semantics</a>, at <a href="https://umass-mlfl.github.io/" target="_blank">Machine Learning and Friends Lunch</a>, UMass Amherst.</li>
    <li>[Aug 2021] Learning 3D Shape Visual Actionable Information for Robotic Manipulation, at <a href="http://svl.stanford.edu/" target="_blank">SVL Reading Group</a>, Stanford.</li>
    <li>[June 2021] Learning 3D Shape Structure and Semantics, at <a href="https://jiajunwu.com/" target="_blank">CogAI Reading Group</a>, Stanford.</li>
    <li>[April 2021] Learning 3D Shape Actionable Information from Simulated Interaction, at <a href="http://www.sfu.ca/visual-computing" target="_blank">SFU VCR (visual computing and robotics) seminar</a>.</li>
    <li>[March 2021] Learning 3D Shape Actionable Information from Simulated Interaction, at <a href="https://www.autodesk.com/research" target="_blank">Autodesk Research</a>.</li>
    <li>[Feb 2021] Where2Act: From Pixels to Actions for Articulated 3D Objects, for <a href="https://www.imperial.ac.uk/matchlab/" target="_blank">the MatchLab</a> at Imperial College London.</li>
    <li>[Feb 2021] Where2Act: From Pixels to Actions for Articulated 3D Objects, for <a href="https://prior.allenai.org/" target="_blank">the PRIOR team</a> at AI2.</li>
    <li>[June 2020] Part-level and Structural 3D Shape Understanding, at <a href="http://geometrylearning.com/" target="_blank">Intelligent Graphics Laboratory (IGL)</a>.</li>
    <li>[June 2020] <a href="https://www.bilibili.com/video/BV1e7411c7kR?p=42" target="_blank">Part-level and Structural Understanding for 3D shape Perception, Synthesis and Editing (in Chinese)</a>, <a href="http://games-cn.org" target="_blank">GAMES: Graphics And Mixed Environment Seminar</a>.</li>
    <li>[April 2019] PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding, at Stanford GCafe Seminar.</li>
</ul>
</div>
<br>

<a name="education"></a>
<h3>Education</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- Stanford -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/stanford.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>CS Ph.D. Candidate, Geometric Computation Group and Artificial Intelligence Lab, Stanford University</b><br/>2016.9 - 2022.6<br/><br/>
        Advisor:
            <a href="http://geometry.stanford.edu/member/guibas/index.html" target='_blank'>Prof. Leonidas J. Guibas</a>
        </p></td>
</tr>
<tr></tr>

<!-- UCSD -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/ucsd.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Visiting Graduate, Su Lab, University of California, San Diego</b><br/>2019.7 - 9<br/><br/>
        Advisor:
            <a href="http://cseweb.ucsd.edu/~haosu/" target='_blank'>Prof. Hao Su</a>
        </p></td>
</tr>
<tr></tr>

<!-- Cornell -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/cornell.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Exchange Research Student, Graphics and Vision Lab, Cornell University</b><br/>2015.6 - 2015.12<br/><br/>
        Research Advisor: <a href="http://www.cs.cornell.edu/~kb/" target="_blank">Prof. Kavita Bala</a><br/>
        Academic Advisor: <a href="http://www.cs.cornell.edu/jeh/" target="_blank">Prof. John E. Hopcroft</a><br/>
        </p></td>
</tr>
<tr></tr>

<!-- SJTU -->
<tr>
    <td width="15%" valign="top"><p><img src="logos/sjtu.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Bachelor of Engineering in Science, ACM Honored Class, Zhiyuan College, Shanghai Jiao Tong University</b><br/>2012.9 - 2016.6<br/><br/>
        Research Advisor: <a href="http://www.math.pku.edu.cn/teachers/zhzhang/" target="_blank">Prof. Zhihua Zhang</a><br>
        Academic Advisor: <a href="http://apex.sjtu.edu.cn/members/yyu" target="_blank">Prof. Yong Yu</a><br/>
        </p></td>
</tr>
<tr></tr>

</table>
</ul>
</div>
<br>

<a name="experiences"></a>
<h3>Working Experiences</h3>
<div class="mainsection">
<ul>
<table width="100%">

<!-- Facebook intern-->
<tr>
    <td width="15%" valign="top"><p><img src="logos/facebook.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Research Intern, Facebook AI Research (<strike>Pittsburgh</strike> Remote from California)</b><br/>2020.6 - 2020.9<br/><br/>
        Mentor:
            <a href="https://shubhtuls.github.io/" target='_blank'>Shubham Tulsiani</a>,
            <a href="http://www.mustafamukadam.com/" target='_blank'>Mustafa Mukadam</a> and 
            <a href="https://www.cs.cmu.edu/~abhinavg/" target='_blank'>Prof. Abhinav Gupta</a>
        </p></td>
</tr>
<tr></tr>

<!-- Autodesk intern-->
<tr>
    <td width="15%" valign="top"><p><img src="logos/autodesk.png" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Research Intern, AI Lab, Autodesk Research</b><br/>2018.6 - 2018.9<br/><br/>
        Mentor:
            <a href="https://autodeskresearch.com/people/mike-haley" target="_blank">Mike Haley</a>
        </p></td>
</tr>
<tr></tr>

<!-- Adobe intern-->
<tr>
    <td width="15%" valign="top"><p><img src="logos/adobe.jpg" width="100" alt="" style="border-style: none" align="top"></p></td>
    <td width="85%" valign="top">
        <p><b>Research Intern, Creative Intelligence Lab, Adobe Research</b><br/>2017.6 - 2017.9<br/><br/>
        Mentors: 
            <a href="http://haoxiang.org/academic-homepage/" target="_blank">Haoxiang Li</a>,
            <a href="https://joonyoung-cv.github.io/" target="_blank">Joon-Young Lee</a>,
            <a href="https://research.adobe.com/person/zhe-lin/" target="_blank">Zhe Lin</a> and 
            <a href="http://www.meyumer.com/" target="_blank">Ersin Yumer</a>
        </p></td>
</tr>

</table>
</div>
</ul>
<br>

<a name="teaching"></a>
<h3>Teaching Experiences</h3>
<div class="section">
<ul>
    <li>Guest Lecturer, Fall 2022, <a href="https://haosulab.github.io/ml-meets-geometry/FA22/index.html" target="_blank">Deep Learning for 3D Data (CSE 291)</a>, UCSD.</li>
    <li>Guest Lecturer, Spring 2022, <a href="https://mhsung.github.io/kaist-cs492a-spring-2022/" target="_blank">Machine Learning for 3D Data (CS 492A)</a>, KAIST.</li>
    <li>Teaching Assistant, Winter 2022, <a href="http://graphics.stanford.edu/courses/cs348n-22-winter/" target="_blank">Neural Representations and Generative Models for 3D Geometry (CS 348n)</a>, Stanford University</li>
    <li>Guest Lecturer, Spring 2021, <a href="http://graphics.stanford.edu/courses/cs233-21-spring/" target="_blank">Geometric and Topological Data Analysis (CS 233)</a>, Stanford University</li>
    <li>Guest Lecturer, Winter 2021, <a href="https://haosulab.github.io/ml-meets-geometry/WI21/index.html" target="_blank">Machine Learning Meets Geometry (CSE 291-I00)</a>, UCSD</li>
    <li>Teaching Assistant, Spring 2020, <a href="http://graphics.stanford.edu/courses/cs233-20-spring/" target="_blank">Geometric and Topological Data Analysis (CS 233)</a>, Stanford University</li>
    <li>Guest Lecturer, Spring 2018, <a href="http://graphics.stanford.edu/courses/cs233-18-spring/" target="_blank">Geometric and Topological Data Analysis (CS 233)</a>, Stanford University</li>
    <li>Teaching Assistant, Fall 2014, Introduction To Computer Science (CS 120), Shanghai Jiao Tong University</li>
</ul>
</div>
<br>

<a name="reviewer"></a>
<h3>Professional Activities</h3>
<div class="section">
<ul>
    <li>Workshop Organizer: 
        AI for 3D Content Generation (AI3DCC)
            [<a href="https://ai3dcc.github.io/" target="_blank">ICCV 2023</a>];
        Structural and Compositional Learning on 3D Data (StruCo3D)
            [<a href="https://geometry.stanford.edu/struco3d/" target="_blank">ICCV 2021</a>, 
            <a href="https://struco3d.github.io/cvpr2023" target="_blank">CVPR 2023</a>];
        Visual Object-oriented Learning meets Interaction: Discovery, Representations, and Applications (VOLI)
            [<a href="https://geometry.stanford.edu/voli/" target="_blank">ECCV 2022</a>];
        3D Vision and Robotics (3DVR)
            [<a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics" target="_blank">CVPR 2023</a>];
        The Rhobin Challenge -- Reconstruction of human-object interaction
            [<a href="https://rhobin-challenge.github.io/" target="_blank">CVPR 2023</a>]
    </li>
    <li>Conference Area Chair: NeurIPS Datasets and Benchmarks Track 2022, 2023</li>
    <li>International Program Committee: Eurographics 2024</li>
    <li>Senior Program Committee Member: AAAI 2023, 2024</li>
    <li>Conference Reviewer: ICML 2021, 2022, 2023; ICLR 2021, 2022, 2023, 2024; NeurIPS 2020, 2021, 2022, 2023; NeurIPS Datasets and Benchmarks Track 2021; CVPR 2020, 2021 (outstanding reviewer), 2022, 2023; ICCV 2019, 2021, 2023; ECCV 2020, 2022; CoRL 2022; RSS 2021, 2023; ICRA 2020; IROS 2021; Siggraph 2021, 2022, 2023; Siggraph Asia 2020, 2021, 2022, 2023; AAAI 2020, 2021, 2022; ACCV 2020; 3DV 2017, 2018, 2019, 2020, 2021, 2022; Pacific Graphics 2020; WACV 2020, 2021, 2022; MVA 2019;</li>
    <li>Workshop Reviewer: SEAI (Simulation Technology for Embodied AI) 2021; CICV (Compositionality in Computer Vision) 2020, 3DRW (3D Reconstruction in the Wild) 2018, 2019; VLEASE (Visual Learning and Embodied Agents in Simulation Environments) 2018;</li>
    <li>Journal Reviewer: Robotics and Automation Letters (RA-L); Transactions on Pattern Analysis and Machine Intelligence (TPAMI); IEEE Transactions on Visualization and Computer Graphics (TVCG); IEEE Transactions on Image Processing (TIP); IEEE Transactions on Multimedia; IEEE Transactions on Robotics (TRO); ACM Transactions on Graphics (TOG); Computational Visual Media (CVM); Signal Processing: Image Communication; Computers &amp; Graphics; Information Fusion; International Journal of Advanced Robotic Systems.</li>
</ul>
</div>
<br>

<a name="honor"></a>
<h3>Honors and Awards</h3>
<div class="section">
<ul>
    <li>[2016-2017] School of Engineering Fellowship, Stanford University</li>
    <li>[Spring 2015] Meritorious Winner, 2015 Mathematical Contest In Modeling (MCM), <a href="resources/2015MCM_paper.pdf" target="_blank">[Paper]</a></li>
    <li>[Fall 2015], National Scholarship, Shanghai Jiao Tong University</li>
    <li>[Fall 2014], National Scholarship, Shanghai Jiao Tong University</li>
    <li>[Fall 2013], KoGuan Scholarship, Shanghai Jiao Tong University</li>
    <li>[Fall 2011], The First Prize, National High School Mathematics Contest, Tianjin, China</li>
    <li>[Fall 2010], The First Prize, National Olympiad in Informatics in Provinces, Tianjin, China</li>
</ul>
</div>
<br>


<hr/>
<div id="footer" style="font-size:10">Kaichun Mo <i>Last updated: Aug, 2022</i></div>
</body>
